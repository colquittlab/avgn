{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this segmentation is a sub-segmentation of the original segmentations provided by the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) create a dataset of syllable waveforms for each isolation segment\n",
    "\n",
    "2) get segmented times for each syllable in that segment\n",
    "\n",
    "3) create a new dataframe with the isolated syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:24.294777Z",
     "start_time": "2019-11-12T01:10:24.267641Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:25.416853Z",
     "start_time": "2019-11-12T01:10:24.297119Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.Parallel(n_jobs=-1, verbose=1)(\n",
    "            joblib.delayed(print)(a) \n",
    "                 for a in range(1000)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:29.543003Z",
     "start_time": "2019-11-12T01:10:25.421738Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.autonotebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import umap\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:29.740188Z",
     "start_time": "2019-11-12T01:10:29.552044Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.Parallel(n_jobs=-1, verbose=1)(\n",
    "            joblib.delayed(print)(a) \n",
    "                 for a in range(1000)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:29.949008Z",
     "start_time": "2019-11-12T01:10:29.742409Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.utils.paths import DATA_DIR, most_recent_subdirectory, ensure_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:30.000770Z",
     "start_time": "2019-11-12T01:10:29.952466Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ID = 'batsong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T19:03:05.594482Z",
     "start_time": "2019-11-12T19:03:05.531644Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a unique datetime identifier for the files output by this notebook\n",
    "DT_ID = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "DT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:30.167437Z",
     "start_time": "2019-11-12T01:10:30.087952Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.utils.hparams import HParams\n",
    "from avgn.dataset import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:30.261903Z",
     "start_time": "2019-11-12T01:10:30.170523Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.signalprocessing.create_spectrogram_dataset import prepare_wav, create_label_df, get_row_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:10:30.365798Z",
     "start_time": "2019-11-12T01:10:30.264627Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams = HParams(\n",
    "    num_mel_bins = 32,\n",
    "    mel_lower_edge_hertz=500,\n",
    "    mel_upper_edge_hertz=120000,\n",
    "    butter_lowcut = 500,\n",
    "    butter_highcut = 120000,\n",
    "    ref_level_db = 20,\n",
    "    min_level_db = -60,\n",
    "    mask_spec = True,\n",
    "    win_length_ms = 0.5,\n",
    "    hop_length_ms = 0.05,\n",
    "    mask_spec_kwargs = {\"spec_thresh\": 0.9, \"offset\": 1e-10},\n",
    "    n_jobs = -1,\n",
    "    verbosity=1,\n",
    "    nex = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:11:01.996949Z",
     "start_time": "2019-11-12T01:10:30.367743Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a dataset object\n",
    "dataset = DataSet(DATASET_ID, hparams = hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:11:02.105925Z",
     "start_time": "2019-11-12T01:11:02.000917Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset.data_files = {i:dataset.data_files[i] for i in list(dataset.data_files.keys())[:5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:11:02.208328Z",
     "start_time": "2019-11-12T01:11:02.108072Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.sample_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:11:02.349584Z",
     "start_time": "2019-11-12T01:11:02.211751Z"
    }
   },
   "outputs": [],
   "source": [
    "len(dataset.data_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset based upon JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:11:02.443528Z",
     "start_time": "2019-11-12T01:11:02.353559Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "n_jobs = -1; verbosity = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:22.340669Z",
     "start_time": "2019-11-12T01:11:02.447321Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=n_jobs, verbose=verbosity) as parallel:\n",
    "    syllable_dfs = parallel(\n",
    "        delayed(create_label_df)(\n",
    "            dataset.data_files[key].data,\n",
    "            hparams=dataset.hparams,\n",
    "            labels_to_retain=[\"context\"],\n",
    "            unit=\"syllables\",\n",
    "            dict_features_to_retain = [],\n",
    "            key = key,\n",
    "        )\n",
    "        for key in tqdm(dataset.data_files.keys())\n",
    "    )\n",
    "syllable_df = pd.concat(syllable_dfs)\n",
    "len(syllable_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.178138Z",
     "start_time": "2019-11-12T01:14:22.342794Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.Parallel(n_jobs=-1, verbose=1)(\n",
    "            joblib.delayed(print)(a) \n",
    "                 for a in range(1000)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.262566Z",
     "start_time": "2019-11-12T01:14:23.180242Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.357017Z",
     "start_time": "2019-11-12T01:14:23.264636Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df.context.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.538282Z",
     "start_time": "2019-11-12T01:14:23.359109Z"
    }
   },
   "outputs": [],
   "source": [
    "from vocalseg.dynamic_thresholding import dynamic_threshold_segmentation\n",
    "from vocalseg.dynamic_thresholding import plot_segmented_spec, plot_segmentations\n",
    "from vocalseg.utils import butter_bandpass_filter, spectrogram, int16tofloat32, plot_spec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save audio df for each context since this is a big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.608895Z",
     "start_time": "2019-11-12T01:14:23.540466Z"
    }
   },
   "outputs": [],
   "source": [
    "n_fft=1024\n",
    "hop_length_ms=.5\n",
    "win_length_ms=4\n",
    "ref_level_db=20\n",
    "pre=0.97\n",
    "min_level_db=-50\n",
    "silence_threshold = 0.02\n",
    "min_silence_for_spec=0.1\n",
    "max_vocal_for_spec=1.0,\n",
    "min_syllable_length_s = 0.01\n",
    "spectral_range = [5000, 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.705617Z",
     "start_time": "2019-11-12T01:14:23.610950Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.799732Z",
     "start_time": "2019-11-12T01:14:23.707734Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.utils.audio import int16_to_float32\n",
    "from avgn.signalprocessing.filtering import butter_bandpass_filter\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.891969Z",
     "start_time": "2019-11-12T01:14:23.801833Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:23.971009Z",
     "start_time": "2019-11-12T01:14:23.894266Z"
    }
   },
   "outputs": [],
   "source": [
    "db_delta = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T19:02:42.697349Z",
     "start_time": "2019-11-12T19:02:42.557127Z"
    }
   },
   "outputs": [],
   "source": [
    "butter_lowcut = 500\n",
    "butter_highcut = 120000\n",
    "def segment_spec_custom(indv, key, start_time, end_time, context, json_dict, save=False, plot=False):\n",
    "    # load wav\n",
    "    wav_loc = dataset.data_files[key].data['wav_loc']\n",
    "    duration = end_time - start_time\n",
    "    data, rate = librosa.core.load(wav_loc, sr=None, offset = start_time, duration = duration)\n",
    "    \n",
    "    # filter data\n",
    "    data = butter_bandpass_filter(data, butter_lowcut, butter_highcut, rate)\n",
    "    \n",
    "    # segment\n",
    "    results = dynamic_threshold_segmentation(\n",
    "        data,\n",
    "        rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length_ms=hop_length_ms,\n",
    "        win_length_ms=win_length_ms,\n",
    "        min_level_db_floor=20,\n",
    "        db_delta=db_delta,\n",
    "        ref_level_db=ref_level_db,\n",
    "        pre=pre,\n",
    "        \n",
    "        min_silence_for_spec=min_silence_for_spec,\n",
    "        max_vocal_for_spec=max_vocal_for_spec,\n",
    "        min_level_db=min_level_db,\n",
    "        silence_threshold=silence_threshold,\n",
    "        verbose=False,\n",
    "        min_syllable_length_s=min_syllable_length_s,\n",
    "        spectral_range=spectral_range,\n",
    "    )\n",
    "    \n",
    "    if results is None:\n",
    "        return\n",
    "    \n",
    "    if plot:\n",
    "        plot_segmentations(\n",
    "            results[\"spec\"],\n",
    "            results[\"vocal_envelope\"],\n",
    "            results[\"onsets\"],\n",
    "            results[\"offsets\"],\n",
    "            hop_length_ms,\n",
    "            rate,\n",
    "            figsize=(100, 5)\n",
    "        )\n",
    "        plt.show()\n",
    "    \n",
    "    # save the results\n",
    "    json_out = DATA_DIR / \"processed\" / (DATASET_ID + \"_segmented\") / DT_ID / \"JSON\" / (\n",
    "        key + \".JSON\"\n",
    "    )\n",
    "    json_dict['context'] = context\n",
    "    json_dict[\"indvs\"][indv][\"syllables\"] = {\n",
    "        \"start_times\": NoIndent(list(results[\"onsets\"])),\n",
    "        \"end_times\": NoIndent(list(results[\"offsets\"])),\n",
    "    }\n",
    "\n",
    "    json_txt = json.dumps(json_dict, cls=NoIndentEncoder, indent=2)\n",
    "    # save json\n",
    "    if save:\n",
    "        ensure_dir(json_out.as_posix())\n",
    "        print(json_txt, file=open(json_out.as_posix(), \"w\"))\n",
    "\n",
    "    #print(json_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T19:02:42.829771Z",
     "start_time": "2019-11-12T19:02:42.714479Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from avgn.utils.json import NoIndent, NoIndentEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T19:02:42.956289Z",
     "start_time": "2019-11-12T19:02:42.851458Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T19:03:03.272361Z",
     "start_time": "2019-11-12T19:02:43.011378Z"
    }
   },
   "outputs": [],
   "source": [
    "iters = [[row.indv, row.key, row.start_time, row.end_time, row.context] for idx, row in tqdm(syllable_df.iterrows(), total = len(syllable_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T19:03:05.528605Z",
     "start_time": "2019-11-12T19:03:03.275370Z"
    }
   },
   "outputs": [],
   "source": [
    "nex = 3\n",
    "joblib.Parallel(n_jobs=1, verbose=11)(\n",
    "            joblib.delayed(segment_spec_custom)( indv, key, start_time, end_time, context, dataset.data_files[key].data.copy(), plot=True) \n",
    "                 for indv, key, start_time, end_time, context in tqdm(iters[:nex])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T01:14:44.748672Z",
     "start_time": "2019-11-12T01:14:44.681383Z"
    }
   },
   "outputs": [],
   "source": [
    "syllable_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-12T19:03:15.665Z"
    }
   },
   "outputs": [],
   "source": [
    "nex = -1\n",
    "n_jobs = 1\n",
    "joblib.Parallel(n_jobs=n_jobs, verbose=11)(\n",
    "            joblib.delayed(segment_spec_custom)(indv, key, start_time, end_time, context, dataset.data_files[key].data.copy(), save = True, plot=False) \n",
    "                 for indv, key, start_time, end_time, context in tqdm(iters[:nex])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
